{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fd8bbf-fadf-4341-86ee-d2005da7a56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import logging\n",
    "import subprocess\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from Scripts.ImageAug import process_dataset  \n",
    "from Scripts.ExtractFeatures import extract_features  \n",
    "from Scripts.Models import evaluate_models  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cf8934-2960-4a80-9dda-11d74fe186c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f18c98-6bab-44ba-82c7-7dd7d9f1eabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_directory_contents(directory):\n",
    "    \"\"\"List the contents of a directory.\"\"\"\n",
    "    logging.info(f\"Contents of {directory}:\")\n",
    "    contents = os.listdir(directory)\n",
    "    for item in contents:\n",
    "        logging.info(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941cd79a-876d-4b53-88eb-956653c47874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_process_dataset(dataset_type, num_augmentations=1):\n",
    "    \"\"\"Create and process the dataset.\"\"\"\n",
    "    os.system(f\"./Scripts/create_dataset.sh {dataset_type}\")\n",
    "    logging.info(\"Dataset creation script executed for %s\", dataset_type)\n",
    "    \n",
    "    dataset_dir = f\"{dataset_type.capitalize()}_Dataset/\"\n",
    "    \n",
    "    if not os.path.exists(dataset_dir):\n",
    "        logging.error(\"Dataset directory not found: %s\", dataset_dir)\n",
    "        raise FileNotFoundError(f\"Dataset directory '{dataset_dir}' does not exist.\")\n",
    "    \n",
    "    list_directory_contents(dataset_dir)\n",
    "    \n",
    "    process_dataset(dataset_dir, num_augmentations=num_augmentations)\n",
    "    logging.info(\"Processing completed for %s\", dataset_dir)\n",
    "    \n",
    "    return 'Augmented_DataSet/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b127bc08-0cc5-4ad1-af75-fe26920106b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_for_models(data_dir):\n",
    "    \"\"\"Extract features using various models.\"\"\"\n",
    "    models = ['ResNet50', 'InceptionV3', 'MobileNetV2', 'DenseNet121', 'EfficientNetB0']\n",
    "    for model in models:\n",
    "        extract_features(model, data_dir)\n",
    "        logging.info(\"Extracted features using %s\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85e9c62-b754-4a67-86ea-d44ea868ca0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models_and_save_results(model_params, output_filename, class_names, dataset_type):\n",
    "    \"\"\"Evaluate models, save results, and save models to directory.\"\"\"\n",
    "    results = evaluate_models(model_params=model_params, class_names=class_names)\n",
    "    \n",
    "    if not os.path.exists(\"Reports\"):\n",
    "        os.makedirs(\"Reports\")\n",
    "    \n",
    "    os.rename(\"Reports/model_results.csv\", output_filename)\n",
    "    logging.info(\"Model results saved to %s\", output_filename)\n",
    "\n",
    "    move_models(dataset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeca105-495f-4ccc-a8c0-d305e6327632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_cm(dir_name):\n",
    "    \"\"\"Move confusion matrices to a directory named after the dataset type.\"\"\"\n",
    "    os.makedirs(f\"Conf_Matrix/{dir_name}\", exist_ok=True)\n",
    "    for file_path in glob.glob(\"Conf_Matrix/*.png\"):\n",
    "        shutil.move(file_path, f\"Conf_Matrix/{dir_name}/\")\n",
    "        print(f'Moved: {file_path} to Conf_Matrix/{dir_name}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20820269-2f33-48d8-8b6d-17f89097604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_models(dir_name):\n",
    "    \"\"\"Move saved models to a directory named after the dataset type.\"\"\"\n",
    "    os.makedirs(f\"Models/{dir_name}\", exist_ok=True)\n",
    "    for file_path in glob.glob(\"Models/*.pkl\"):  \n",
    "        shutil.move(file_path, f\"Models/{dir_name}/\")\n",
    "        print(f'Moved: {file_path} to Models/{dir_name}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0954692d-7f46-4e6f-9df5-7909e1a47bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    list_directory_contents(\"Original_DataSet/\")\n",
    "\n",
    "\n",
    "    model_params = {\n",
    "    SVC: [\n",
    "        {'kernel': 'rbf', 'C': 0.1, 'gamma': 'scale'},\n",
    "        {'kernel': 'rbf', 'C': 0.1, 'gamma': 'auto'},\n",
    "        {'kernel': 'rbf', 'C': 1, 'gamma': 'scale'},\n",
    "        {'kernel': 'rbf', 'C': 1, 'gamma': 'auto'},\n",
    "        {'kernel': 'rbf', 'C': 10, 'gamma': 'scale'},\n",
    "        {'kernel': 'rbf', 'C': 10, 'gamma': 'auto'},\n",
    "        {'kernel': 'linear', 'C': 0.1},\n",
    "        {'kernel': 'linear', 'C': 1},\n",
    "        {'kernel': 'linear', 'C': 10}\n",
    "    ],\n",
    "    DecisionTreeClassifier: [\n",
    "        {'max_depth': None, 'min_samples_split': 2},\n",
    "        {'max_depth': None, 'min_samples_split': 5},\n",
    "        {'max_depth': None, 'min_samples_split': 10},\n",
    "        {'max_depth': 5, 'min_samples_split': 2},\n",
    "        {'max_depth': 5, 'min_samples_split': 5},\n",
    "        {'max_depth': 5, 'min_samples_split': 10}\n",
    "    ],\n",
    "    RandomForestClassifier: [\n",
    "        {'n_estimators': 100, 'max_depth': None, 'random_state': 42, 'max_features': 'sqrt'},\n",
    "        {'n_estimators': 100, 'max_depth': None, 'random_state': 42, 'max_features': 'log2'},\n",
    "        {'n_estimators': 200, 'max_depth': 10, 'min_samples_split': 2, 'max_features': 'sqrt'},\n",
    "        {'n_estimators': 200, 'max_depth': 10, 'min_samples_split': 5, 'max_features': 'sqrt'},\n",
    "        {'n_estimators': 200, 'max_depth': None, 'min_samples_split': 2, 'max_features': 'sqrt'},\n",
    "        {'n_estimators': 200, 'max_depth': None, 'min_samples_split': 5, 'max_features': 'sqrt'}\n",
    "    ],\n",
    "    KNeighborsClassifier: [\n",
    "        {'n_neighbors': 3, 'weights': 'uniform'},\n",
    "        {'n_neighbors': 3, 'weights': 'distance'},\n",
    "        {'n_neighbors': 5, 'weights': 'uniform'},\n",
    "        {'n_neighbors': 5, 'weights': 'distance'},\n",
    "        {'n_neighbors': 7, 'weights': 'uniform'},\n",
    "        {'n_neighbors': 7, 'weights': 'distance'}\n",
    "    ]\n",
    "    }\n",
    "\n",
    "\n",
    "    datasets = [\n",
    "        {'type': 'binary', 'filename': 'Reports/model_results_binary.csv', 'class_names': ['Healthy', 'Reject']},\n",
    "        {'type': 'multiclass1', 'filename': 'Reports/model_results_three_class.csv', 'class_names': ['Ripe', 'Unripe', 'Reject']},\n",
    "        {'type': 'multiclass2', 'filename': 'Reports/model_results_four_class.csv', 'class_names': ['Ripe', 'Unripe', 'Old', 'Damaged']}\n",
    "    ]\n",
    "\n",
    "    for dataset in datasets:\n",
    "        data_dir = create_and_process_dataset(dataset['type'])\n",
    "        extract_features_for_models(data_dir)\n",
    "        evaluate_models_and_save_results(\n",
    "            model_params=model_params,\n",
    "            output_filename=dataset['filename'],\n",
    "            class_names=dataset['class_names'],\n",
    "            dataset_type=dataset['type']\n",
    "        )\n",
    "        move_cm(dataset['type'])\n",
    "\n",
    "    logging.info(\"#################### Completed ##########################\")\n",
    "    command = \"rm -rf Augmented_DataSet Binary_Dataset Features Multiclass1_Dataset Multiclass2_Dataset\"\n",
    "\n",
    "    try:\n",
    "        subprocess.run(command, shell=True, check=True)\n",
    "        print(\"Directories removed successfully.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27384fd-aca2-4f3f-910d-aa3c3aba4a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
